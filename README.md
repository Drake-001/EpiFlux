Understanding the cognition and biases of AI models is a technological challenge and ethical imperative. With this in mind, my project dives deep into the realms of neural epistemology, seeking to decipher how computational systems, analogous to sentient beings, form and stabilize beliefs. Using PyTorch, I will design a primary neural network that interprets and synthesizes beliefs from a stream of input data. This will be complemented by a secondary metacognitive network, which critically reflects upon the belief structures formed by the primary network. This introspective layer will assess belief strength, consistency, and potential biases.

This is inspired by the philosopher Richard Rorty’s claims about the contingency of language and belief formation, bringing forth the idea that beliefs, whether human or artificial, are not merely reflections of an external truth, but constructs influenced by the medium and process of their formation. One could see how the secondary network, rather than uncovering an “objective truth” about the primary’s knowledge, might itself manifest contingent interpretations, shaped uniquely by its own architecture and learning experience. 

Beyond the philosophical, the findings from this project could shed light on the inherent subjectivity of AI systems, guiding the tech industry’s approach to model interpretability, bias assessment, and ethical AI deployment.
